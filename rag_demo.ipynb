{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## genAI RAG Demo\n",
    "\n",
    "This notebook is based of [this tutorial](https://github.com/jedi4ever/learning-llms-and-genai-for-dev-sec-ops) repo from Patrick Debois (thanks Patrick!!)\n",
    "\n",
    "- We'll be using the Langchain <https://github.com/langchain-ai/langchain> for our coding.\n",
    "- We will be using Python but it also exists in a Typescript variant.\n",
    "\n",
    "- These framework are a great way to track new ideas and examples through their documentation.\n",
    "\n",
    "We recommend using [visual studio code](https://code.visualstudio.com/) for an editor and either running a local [virtual environment](https://code.visualstudio.com/docs/python/environments) or, if you're interested in [Docker](https://www.docker.com/), using a [dev container](https://code.visualstudio.com/docs/devcontainers/containers) (a devcontainer.json file is included with this repo).\n",
    "\n",
    "This tutorial will walk through a basic RAG application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Hello OpenAI: Installing dependencies for [\"Hello, World!\"](https://en.wikipedia.org/wiki/%22Hello,_World!%22_program)\n",
    "If we're in a new virtual environment (see above), our first step install langchain and open AI dependencies with pip, the python package manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-openai langchain pypdf langchain-community chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These frameworks allow the use of multiple different llms. Here we'll use OpenAI as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using OpenAI requires an API key `OPENAI_API_KEY` to be loaded. Python can use a `.env` file to store these values. Make sure you have either your personal or your hackathon team's OpenAI API key added to the .env file. VS code will automatically pick up the .env file, otherwise you may need to run the command below to install a package to load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the env loaded we make the simplest request by instantiating the OpenAI API and sending it a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "llm = OpenAI()\n",
    "answer = llm.invoke(\"Hello world\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we try this example a few times we'll see that the output is not consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()\n",
    "for i in range(1,6):\n",
    "    answer = llm.invoke(\"Hello world\")\n",
    "    print(f\"A{i}:{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to make the result more predictable we set the `temperature` option. This is the degree of randomness we allow the model to take in it's predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "for i in range(1,6):\n",
    "    answer = llm.invoke(\"Hello world\")\n",
    "    print(f\"A{i}:{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some llms allow you to stream the results character by character. Feel free to change the prompt or the temperature and run the notebook cell below a few times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(streaming=True, temperature=0)\n",
    "answer=llm.invoke(\"Where in the world is John Willis? try to answer verbose\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. A simple OpenAI chat bot\n",
    "\n",
    "The last example let us ask openAI a question in plain english. Now we'll explore setting up a simple chat bot.\n",
    "\n",
    "Chat models typically operate using a set of messages they send to the LLM.\n",
    "- The System message is what sets the tone or the initial behavior of the model\n",
    "- The Human message that asks the question\n",
    "- The AI Message that is the response of the AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a chat model instead of a regular LLM\n",
    "from langchain_openai import ChatOpenAI\n",
    "chat = ChatOpenAI()\n",
    "\n",
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    AIMessage,\n",
    "    HumanMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that provides security advice.\"),\n",
    "    HumanMessage(content=\"What makes devOps secure?\")\n",
    "]\n",
    "answer = chat.invoke(messages)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create a template for the prompts we are sending.\n",
    "Related Langchain lesson: <https://python.langchain.com/docs/concepts/prompt_templates/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Tell me a {adjective} joke about {content}.\"\n",
    ")\n",
    "prompt = prompt_template.format(adjective=\"funny\", content=\"AI\")\n",
    "\n",
    "from langchain_openai import OpenAI\n",
    "llm = OpenAI(temperature=0)\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to a prompt template , we can also use chat prompt templates more optimized for conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "    (\"human\", \"Hello, how are you doing?\"),\n",
    "    (\"ai\", \"I'm doing well, thanks!\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])\n",
    "\n",
    "messages = template.format_messages(\n",
    "    name=\"Bob\",\n",
    "    user_input=\"What is your name?\"\n",
    ")\n",
    "\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Document Loading \n",
    "\n",
    "Now that we have the prompts loaded and the llm answering our questions, we dive into mixing the llm with our own content. This is useful because llms usually have a knowledge date cut off and also we don't want private information to leak into public llms. \n",
    "\n",
    "An important technique for this is called **RAG: Retrieval Augmented Generation**.\n",
    "\n",
    "Langchain has a set of documentloader to load & parse different formats.\n",
    "\n",
    "First, we need to install some more python dependencies. We'll be using **pypdf** here, but there a _lot_ of other options available on [langchain's website](https://python.langchain.com/docs/how_to/document_loader_pdf/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"PDFs/NIST.AI.100-1.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "# these are zero indexed, so \"2\" here is really page 3.\n",
    "pages[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also load all the PDFs in the directory at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    \"./\",\n",
    "    glob=\"PDFs/**/*.*\",\n",
    "    use_multithreading=True,\n",
    "    max_concurrency=4,\n",
    "    show_progress=True,\n",
    "    loader_cls=PyPDFLoader\n",
    ")\n",
    "pdf_pages = loader.load()\n",
    "print('number of PDF pages:',len(pdf_pages))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chunking & Splitting\n",
    "\n",
    "Our PDF pages might be too long to fit in the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\n",
    "\n",
    "To handle this weâ€™ll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant bits of the blog post at run time.\n",
    "\n",
    "Chunking and splitting strategies can drastically improve the quality of results for a RAG application. This is a more advanced topic within genAI so we won't explore it further here, but your team may want to investigate how chunking and splitting could impact ther performance and quality of your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(pdf_pages)\n",
    "\n",
    "\n",
    "all_splits[10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Embeddings & Vector stores\n",
    "\n",
    "Now that we have the PDFs loaded, parsed, chunked, and split they're starting to sound like and order of waffle house hashbrowns. The next step is to calculate an \"embedding\" of the texts. \n",
    "\n",
    "An embedding in its simplest form is a multi dimensional mathematical vector that calculates the similarity of a piece of text. Topics and content that are similar have vectors that are close together. \n",
    "\n",
    "Embeddings enhance traditional search engines that use keywords or synonyms. Both have their place, but in this example we'll use embeddings as a way to calculate the proximity of texts.\n",
    "\n",
    "Why similarity you might ask? Well, RAG allows us to provide an LLM with more context about a subject when we ask it a question. This context is done by looking up documents that have similarity and giving that information. So in this section we're laying the groundwork for that end-goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "\n",
    "embeddings = embeddings_model.embed_documents(\n",
    "    [\n",
    "        \"Hi there!\",\n",
    "        \"Oh, hello!\",\n",
    "        \"What's your name?\",\n",
    "        \"My friends call me World\",\n",
    "        \"Hello World!\"\n",
    "    ]\n",
    ")\n",
    "print(f\"Embeddings calculated: {len(embeddings)}\")\n",
    "print(f\"Vectorsize of first embedding :{len(embeddings[0])}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectors are just like vectors found in some math or physics courses, so they indicate a magnitude (number), and a direction(+/- at a minimum). Most real world vectors have about 3 dimensions. These vectors have a lot more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of calculating these embeddings everytime, we'll store these vectors into a **vector database** or a cache. Caching is another advanced topic outside the scope of this quickstart that your team may want to investigate further.\n",
    "\n",
    "In many cases, vector databases also provide the service to calculate the embeddings using the model you select. For this example we'll be using [Chroma DB](https://www.trychroma.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "# Set the embeddings function\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "\n",
    "# Vectory database will calculate them using the embeddings_model provided\n",
    "# and store the embeddings for each doc in it's database\n",
    "db = Chroma.from_documents(all_splits, embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "query = \"devOps\"\n",
    "docs = db.similarity_search_with_relevance_scores(query, k=4, score_threshold=0.7)\n",
    "pprint(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrolling all the way to the right to check the metadata, we can see The code returned a few splits that point to either the table of contents or page 4 (3+1) of the Great_Dane_Maintenance_Manual where the the freezing weather maintenance is discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Question Answering Retrival Augmented Generation (QA RAG)\n",
    "\n",
    "We're almost there!\n",
    "- We loaded the documents.\n",
    "- We've split them in to relevant parts (splitters)\n",
    "- We've had the vector database calculate the embeddings and store the related parts\n",
    "\n",
    "Related Langchain example: https://python.langchain.com/docs/use_cases/question_answering/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now We'll set up our QA chain with our LLM and our DB as a retriever. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What makes devOps secure?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion & Next Steps\n",
    "\n",
    "- Can our QA RAG bot answer questions more accurately than vectore simularity search alone?\n",
    "- How might the data included or excluded from the vectorstore along with how it's split and chunked affect the results?\n",
    "\n",
    "Thanks, and happy hacking!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
